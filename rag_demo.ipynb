{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e94fa5b",
   "metadata": {},
   "source": [
    "# RAG Demo – Airline Policy Knowledge (Clean Ordered Version)\n",
    "\n",
    "This notebook implements a minimal, student‑friendly Retrieval‑Augmented Generation (RAG) pipeline over a tiny airline policy / ticketing corpus.\n",
    "\n",
    "Ordered Sections:\n",
    "1. Requirements\n",
    "2. Imports\n",
    "3. Manual Gemini Key (optional) / LLM Mode Toggle\n",
    "4. Ingestion (read PDFs + CSV)  – falls back to a synthetic mini corpus if `dummy_files/` not found\n",
    "5. Preprocessing\n",
    "6. Embeddings (TF‑IDF fallback; SentenceTransformer optional if installed)\n",
    "7. Build Simple Vector Index (in‑memory matrix or FAISS if available)\n",
    "8. Retrieval Test\n",
    "9. RAG Assembly (prompt build + generation)\n",
    "10. End-to-End Example\n",
    "11. Sanity Checks\n",
    "12. Notes / Next Steps\n",
    "\n",
    "Run top → bottom. Everything works offline by default (mock LLM + TF‑IDF). Enable Gemini only if you manually paste an API key in Section 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b3289c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity passed: True\n"
     ]
    }
   ],
   "source": [
    "# 11. Sanity Checks\n",
    "\n",
    "def sanity(r: Dict[str, Any]):\n",
    "    assert r['docs'], 'No retrieval'\n",
    "    assert isinstance(r['answer'], str) and r['answer'].strip(), 'Empty answer'\n",
    "    for d in r['docs']:\n",
    "        assert d['id'] in r['prompt'], 'Doc id missing in prompt'\n",
    "    assert len(r['prompt']) > 40, 'Prompt too short'\n",
    "    return True\n",
    "\n",
    "print('Sanity passed:', sanity(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b406e98",
   "metadata": {},
   "source": [
    "## 11. Sanity Checks\n",
    "Lightweight asserts confirm the pipeline produces non-empty, coherent artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e6ad2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION:\n",
      " How are cancellations handled and what about baggage limits?\n",
      "\n",
      "RETRIEVED:\n",
      "- ticketing_info_row0 (score=0.000)\n",
      "- ticketing_info_row1 (score=0.000)\n",
      "- ticketing_info_row2 (score=0.000)\n",
      "- ticketing_info_row3 (score=0.000)\n",
      "\n",
      "ANSWER:\n",
      " (Mock) Answer ONLY from context; if unknown say 'unsure'. QUESTION: How are cancellations handled and what about baggage limits? ANSWER:\n"
     ]
    }
   ],
   "source": [
    "# 10. End-to-End Example\n",
    "question = \"How are cancellations handled and what about baggage limits?\"\n",
    "result = rag(question, k=4)\n",
    "print(\"QUESTION:\\n\", result['question'])\n",
    "print(\"\\nRETRIEVED:\")\n",
    "for d in result['docs']:\n",
    "    print(f\"- {d['id']} (score={d['score']:.3f})\")\n",
    "print(\"\\nANSWER:\\n\", result['answer'][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fba5b",
   "metadata": {},
   "source": [
    "## 10. End-to-End Example\n",
    "Ask a composite question and view retrieved documents + answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "909cefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Mock) You are a concise assistant. Use only the provided CONTEXT. If the answer is unknown, say you are unsure. QUESTION: What is the baggage allowance? ANSWER\n"
     ]
    }
   ],
   "source": [
    "# 9. RAG Assembly (prompt build + generation)\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a concise assistant. Use only the provided CONTEXT. If the answer is unknown, say you are unsure.\\n\"\n",
    "    \"CONTEXT:\\n{context}\\n\\nQUESTION: {question}\\nANSWER:\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, docs: List[Dict[str, Any]]) -> str:\n",
    "    ctx_blocks = [f\"[Doc {d['id']}]\\n{d['content']}\" for d in docs]\n",
    "    context = \"\\n\\n\".join(ctx_blocks)\n",
    "    return PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "def rag(question: str, k: int = 3) -> Dict[str, Any]:\n",
    "    docs = retrieve(question, k=k)\n",
    "    prompt = build_prompt(question, docs)\n",
    "    answer = llm.generate(prompt)\n",
    "    return {\"question\": question, \"answer\": answer, \"docs\": docs, \"prompt\": prompt}\n",
    "\n",
    "example = rag(\"What is the baggage allowance?\", k=3)\n",
    "print(example['answer'][:160])\n",
    "assert example['docs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8e7d7",
   "metadata": {},
   "source": [
    "## 9. RAG Assembly\n",
    "Combine retrieval + prompt building + LLM generation into a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaa2ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Mock Answer) CONTEXT: baggage allowance varies. QUESTION: What varies?\n"
     ]
    }
   ],
   "source": [
    "# 8. LLM Client (Mock or Gemini)\n",
    "class LLMClient:\n",
    "    def __init__(self, use_gemini: bool = False):\n",
    "        self.use_gemini = use_gemini\n",
    "        if self.use_gemini and (not HAS_GEMINI_LIB or not manual_gemini_key.strip()):\n",
    "            raise RuntimeError(\"Gemini requested but library or key missing.\")\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.use_gemini:\n",
    "            # Heuristic: pick lines with keywords from the question tail\n",
    "            question_part = prompt.split('QUESTION:')[-1].lower()\n",
    "            keywords = [w for w in re.split(r\"[^a-z0-9]+\", question_part) if len(w) > 4][:6]\n",
    "            lines = [l.strip() for l in prompt.split('\\n') if l.strip()]\n",
    "            chosen = [l for l in lines if any(k in l.lower() for k in keywords)][:4]\n",
    "            synthesis = ' '.join(chosen) or lines[-1]\n",
    "            return f\"(Mock Answer) {synthesis[:400]}\".strip()\n",
    "        model = genai.GenerativeModel(\"gemini-pro\")\n",
    "        resp = model.generate_content(prompt)\n",
    "        return getattr(resp, 'text', str(resp))\n",
    "\n",
    "llm = LLMClient(use_gemini=use_gemini)\n",
    "print(llm.generate(\"CONTEXT: baggage allowance varies. QUESTION: What varies?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4489e",
   "metadata": {},
   "source": [
    "## 8. LLM Client (Mock or Gemini)\n",
    "A tiny wrapper: mock mode extracts salient lines; Gemini mode calls the API if enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "856859f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample retrieval: [('ticketing_info_row0', 0.0), ('ticketing_info_row1', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# 7. Retrieval Function\n",
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    return l2_normalize(embed([clean(q)])).astype(\"float32\")\n",
    "\n",
    "def retrieve(query: str, k: int = 3):\n",
    "    qv = embed_query(query)\n",
    "    if HAS_FAISS and index is not None:\n",
    "        scores, idxs = index.search(qv, k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], idxs[0]):\n",
    "            doc = documents[idx]\n",
    "            results.append({\"id\": doc['id'], \"score\": float(score), \"content\": doc['content']})\n",
    "        return results\n",
    "    # brute force\n",
    "    sims = norm_embeddings @ qv.T\n",
    "    sims = sims.ravel()\n",
    "    order = np.argsort(-sims)[:k]\n",
    "    return [{\"id\": documents[i]['id'], \"score\": float(sims[i]), \"content\": documents[i]['content']} for i in order]\n",
    "\n",
    "_test = retrieve(\"baggage allowance\", k=2)\n",
    "print(\"Sample retrieval:\", [(r['id'], round(r['score'],3)) for r in _test])\n",
    "assert _test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b57991",
   "metadata": {},
   "source": [
    "## 7. Retrieval Function\n",
    "Embed the query, search top-k by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2711ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS not available; using matrix multiply for similarity.\n"
     ]
    }
   ],
   "source": [
    "# 6. Build Simple Vector Index (normalize + optional FAISS)\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    return x / (np.linalg.norm(x, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "norm_embeddings = l2_normalize(embeddings)\n",
    "if HAS_FAISS:\n",
    "    index = faiss.IndexFlatIP(norm_embeddings.shape[1])\n",
    "    index.add(norm_embeddings)\n",
    "    print(\"FAISS index built (ntotal=\", index.ntotal, \")\")\n",
    "else:\n",
    "    index = None\n",
    "    print(\"FAISS not available; using matrix multiply for similarity.\")\n",
    "assert norm_embeddings.shape == embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa205ee",
   "metadata": {},
   "source": [
    "## 6. Vector Index\n",
    "We normalize vectors and build either a FAISS index (if available) or keep the matrix for brute-force similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1fd155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (83, 513)\n"
     ]
    }
   ],
   "source": [
    "# 5. Embeddings (SentenceTransformer optional, TF-IDF fallback)\n",
    "class TFIDFEmbedder:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        from collections import Counter\n",
    "        self.tokenizer = lambda s: [w for w in re.split(r\"[^a-z0-9]+\", s.lower()) if w]\n",
    "        token_lists = [self.tokenizer(t) for t in texts]\n",
    "        df = Counter()\n",
    "        for toks in token_lists:\n",
    "            for t in set(toks):\n",
    "                df[t] += 1\n",
    "        self.vocab = {w: i for i, (w, _) in enumerate(df.items())}\n",
    "        n = len(texts)\n",
    "        self.idf = {w: math.log((1 + n) / (1 + c)) + 1 for w, c in df.items()}\n",
    "\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        m = np.zeros((len(texts), len(self.vocab)), dtype=\"float32\")\n",
    "        for row, text in enumerate(texts):\n",
    "            toks = self.tokenizer(text)\n",
    "            if not toks:\n",
    "                continue\n",
    "            tf = {}\n",
    "            for t in toks:\n",
    "                tf[t] = tf.get(t, 0) + 1\n",
    "            for t, c in tf.items():\n",
    "                if t in self.vocab:\n",
    "                    col = self.vocab[t]\n",
    "                    m[row, col] = (c / len(toks)) * self.idf[t]\n",
    "            norm = np.linalg.norm(m[row]) + 1e-12\n",
    "            m[row] /= norm\n",
    "        return m\n",
    "\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "if HAS_ST:\n",
    "    st_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    def embed(texts: List[str]) -> np.ndarray:\n",
    "        v = st_model.encode(texts, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False)\n",
    "        return v.astype(\"float32\")\n",
    "else:\n",
    "    tfidf_model = TFIDFEmbedder([d['cleaned'] for d in documents])\n",
    "    def embed(texts: List[str]) -> np.ndarray:  # type: ignore\n",
    "        return tfidf_model.encode(texts)\n",
    "\n",
    "corpus_texts = [d['cleaned'] for d in documents]\n",
    "embeddings = embed(corpus_texts)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "assert embeddings.shape[0] == len(documents)\n",
    "assert embeddings.shape[1] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17dfa9",
   "metadata": {},
   "source": [
    "## 5. Embeddings\n",
    "Use SentenceTransformer if present; otherwise a lightweight TF-IDF style fallback embedder (deterministic, zero external calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62b5ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview: airline baggage policy (effective 2025 q3) cabin (carry-on): each passenger may bring one  ...\n"
     ]
    }
   ],
   "source": [
    "# 4. Preprocessing\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "for d in documents:\n",
    "    d[\"cleaned\"] = clean(d[\"content\"])\n",
    "print(\"Preview:\", documents[0]['cleaned'][:90], '...')\n",
    "assert all('cleaned' in d for d in documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c833f4",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    "Minimal cleaning: lowercase, collapse whitespace. (For larger corpora you might remove boilerplate, HTML, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0491224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPDF2 not installed; using embedded policy placeholders instead of PDF extraction.\n",
      "Policies: 3, Tickets: 80, Total docs: 83\n",
      "Sample doc IDs: ['policy_baggage', 'policy_cancellation', 'policy_terms', 'ticket_2000', 'ticket_2001']\n",
      "Detected fare classes in tickets: ['BASIC', 'BUSINESS', 'FLEX', 'STANDARD']\n"
     ]
    }
   ],
   "source": [
    "# 3. Ingestion (PDF + CSV + Generated Policies & Tickets with fallback)\n",
    "\"\"\"Build a richer mixed corpus:\n",
    "1. Try to read real PDFs & CSVs from dummy_files/.\n",
    "2. Inject full-text policy documents (multi-paragraph) for baggage, cancellations, terms.\n",
    "3. Generate a set of synthetic natural-language ticket records (more query-friendly than raw CSV rows).\n",
    "If dummy_files/ is absent, fall back to a tiny synthetic set.\n",
    "\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_DIR = Path(\"dummy_files\")\n",
    "policy_docs = []\n",
    "raw_ticket_rows = []\n",
    "\n",
    "# 3.1 Read PDFs if possible\n",
    "if DATA_DIR.exists():\n",
    "    if HAS_PDF:\n",
    "        for pdf_path in sorted(DATA_DIR.glob(\"*.pdf\")):\n",
    "            try:\n",
    "                reader = PdfReader(str(pdf_path))\n",
    "                pages = [p.extract_text() or \"\" for p in reader.pages]\n",
    "                text = \"\\n\".join(pages)\n",
    "                policy_docs.append({\"id\": f\"pdf_{pdf_path.stem}\", \"content\": text.strip()})\n",
    "            except Exception as e:\n",
    "                print(f\"PDF read fail {pdf_path.name}: {e}\")\n",
    "    else:\n",
    "        # Provide placeholder rich text if we cannot parse PDFs\n",
    "        print(\"PyPDF2 not installed; using embedded policy placeholders instead of PDF extraction.\")\n",
    "else:\n",
    "    print(\"dummy_files/ not found; using synthetic fallback only.\")\n",
    "\n",
    "# 3.2 Add curated realistic policy texts (always add so we have semantic material)\n",
    "policy_texts = {\n",
    "    \"policy_baggage\": \"\"\"\n",
    "Airline Baggage Policy (Effective 2025 Q3)\n",
    "\n",
    "Cabin (Carry-On): Each passenger may bring one cabin bag up to 7 kg and a personal item (laptop bag or small handbag). Oversized cabin items must be checked at the gate and may incur an oversized handling fee.\n",
    "\n",
    "Checked Allowance: Standard economy fares include 15 kg total checked baggage on domestic routes and 23 kg on international routes. FLEX and BUSINESS fares include an additional 10 kg allowance. Any single bag over 32 kg will not be accepted due to safety restrictions.\n",
    "\n",
    "Excess & Oversize: Excess weight beyond the included allowance is charged per kg at the prevailing airport rate. Oversize sports equipment (e.g., surfboards) must be pre-declared and may attract a fixed handling fee.\n",
    "\n",
    "Prohibited & Restricted Items: Lithium batteries, power banks, and electronic cigarettes must be carried in cabin baggage only. Sharp objects, flammable materials, and loose ammunition are strictly prohibited. Fragile items should be cushioned; the airline is not liable for minor cosmetic damage to suitcases.\n",
    "\n",
    "Delayed / Damaged Baggage: Report issues at the baggage service desk before exiting the arrival hall. Interim purchase reimbursements require original receipts and are capped at a daily limit.\n",
    "\"\"\".strip(),\n",
    "    \"policy_cancellation\": \"\"\"\n",
    "Ticket Change & Cancellation Policy (Updated July 2025)\n",
    "\n",
    "24-Hour Cooling-Off: Bookings made directly through our website can be fully refunded if canceled within 24 hours, provided departure is at least 7 days away.\n",
    "\n",
    "Fare Classes:\n",
    "- BASIC: Non-refundable after cooling-off. Changes not permitted or require full reissue at current fare.\n",
    "- STANDARD: Changes allowed with a change fee plus any fare difference. Partial refund (taxes only) on cancellation.\n",
    "- FLEX: Unlimited date changes (fare difference applies). Cancellation allowed with moderate fee up to 2 hours before departure.\n",
    "- BUSINESS: Fully refundable and changeable until departure; no change fees (fare difference may apply).\n",
    "\n",
    "No-Show: Failure to check in before cut-off voids remaining flight coupons; only unused refundable taxes are returned (if applicable).\n",
    "\n",
    "Irregular Operations: If the airline cancels or significantly delays a flight (over 3 hours), passengers may request rebooking, voucher credit, or full refund regardless of fare class.\n",
    "\"\"\".strip(),\n",
    "    \"policy_terms\": \"\"\"\n",
    "General Conditions of Carriage (Excerpt)\n",
    "\n",
    "Check-In Deadlines: Domestic flights close 45 minutes prior; international flights close 60 minutes prior. Boarding gates typically close 20 minutes before departure.\n",
    "\n",
    "Travel Documents: Passengers are responsible for valid passports, visas, and health certificates. Denied boarding due to documentation issues does not entitle the traveler to a refund beyond fare rules.\n",
    "\n",
    "Safety & Conduct: Crew instructions must be followed at all times. Abusive or disruptive behavior can result in denied boarding or diversion costs being charged to the passenger.\n",
    "\n",
    "Liability Limits: Compensation for baggage loss or damage is limited per kilogram unless a declared value was purchased. The airline is not liable for indirect or consequential losses.\n",
    "\n",
    "Data & Privacy: Booking data may be processed in jurisdictions with differing data protection standards. See full privacy policy on our website.\n",
    "\"\"\".strip(),\n",
    "}\n",
    "\n",
    "for pid, text in policy_texts.items():\n",
    "    policy_docs.append({\"id\": pid, \"content\": text})\n",
    "\n",
    "# 3.3 Read raw CSV ticket rows (optional)\n",
    "if DATA_DIR.exists():\n",
    "    for csv_path in sorted(DATA_DIR.glob(\"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            raw_ticket_rows.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"CSV read fail {csv_path.name}: {e}\")\n",
    "\n",
    "# 3.4 Generate enriched natural-language ticket records\n",
    "FIRST_NAMES = [\"Aarav\", \"Diya\", \"Rohan\", \"Maya\", \"Liam\", \"Aisha\", \"Noah\", \"Ishita\", \"Karan\", \"Eva\", \"Sofia\", \"Omar\", \"Priya\", \"Zara\", \"Leo\"]\n",
    "LAST_NAMES = [\"Patel\", \"Sharma\", \"Das\", \"Khan\", \"Verma\", \"Iyer\", \"Nair\", \"Fernandes\", \"Roy\", \"Singh\", \"Mehta\", \"Ghosh\"]\n",
    "ORIGINS = [\"BLR\", \"DEL\", \"BOM\", \"HYD\", \"MAA\", \"CCU\", \"IXC\", \"PNQ\", \"GOI\"]\n",
    "DESTS = [\"DXB\", \"SIN\", \"DOH\", \"BKK\", \"LHR\", \"JFK\", \"MAA\", \"BLR\", \"BOM\"]\n",
    "FARES = [\"BASIC\", \"STANDARD\", \"FLEX\", \"BUSINESS\"]\n",
    "STATUSES = [\"Booked\", \"Cancelled\", \"Completed\", \"Delayed\"]\n",
    "\n",
    "BAGGAGE_BASE = {\"BASIC\": 15, \"STANDARD\": 15, \"FLEX\": 23, \"BUSINESS\": 32}\n",
    "CHANGE_POLICY = {\n",
    "    \"BASIC\": \"no changes after booking except within 24h cooling-off\",\n",
    "    \"STANDARD\": \"changes allowed with fee\",\n",
    "    \"FLEX\": \"unlimited date changes (fare difference applies)\",\n",
    "    \"BUSINESS\": \"fully flexible, no change fees\",\n",
    "}\n",
    "CANCEL_POLICY = {\n",
    "    \"BASIC\": \"non-refundable beyond taxes\",\n",
    "    \"STANDARD\": \"partial refund (taxes) plus fee\",\n",
    "    \"FLEX\": \"refund with moderate fee\",\n",
    "    \"BUSINESS\": \"fully refundable\",\n",
    "}\n",
    "\n",
    "NUM_TICKETS = 80  # adjustable\n",
    "start_date = datetime(2025, 6, 1)\n",
    "\n",
    "def make_ticket(i: int) -> dict:\n",
    "    fn = random.choice(FIRST_NAMES)\n",
    "    ln = random.choice(LAST_NAMES)\n",
    "    origin = random.choice(ORIGINS)\n",
    "    dest = random.choice([d for d in DESTS if d != origin])\n",
    "    depart = start_date + timedelta(days=random.randint(0, 220), hours=random.randint(0, 23), minutes=random.choice([0,15,30,45]))\n",
    "    fare = random.choices(FARES, weights=[0.35, 0.30, 0.25, 0.10])[0]\n",
    "    status = random.choices(STATUSES, weights=[0.65, 0.1, 0.2, 0.05])[0]\n",
    "    baggage_allow = BAGGAGE_BASE[fare]\n",
    "    ticket_id = 2000 + i\n",
    "    pnr = ''.join(random.choices('ABCDEFGHJKMNPQRSTUVWXYZ23456789', k=6))\n",
    "    base_price = {\"BASIC\": 3200, \"STANDARD\": 5200, \"FLEX\": 8600, \"BUSINESS\": 14500}[fare]\n",
    "    price = base_price + random.randint(-400, 1200)\n",
    "    narrative = (\n",
    "        f\"Ticket {ticket_id} (PNR {pnr}) for passenger {fn} {ln} travels from {origin} to {dest} on {depart.strftime('%Y-%m-%d %H:%M')} \"\n",
    "        f\"in {fare} class. Included checked baggage allowance {baggage_allow} kg. Change policy: {CHANGE_POLICY[fare]}. \"\n",
    "        f\"Cancellation: {CANCEL_POLICY[fare]}. Current status: {status}.\"\n",
    "    )\n",
    "    return {\"id\": f\"ticket_{ticket_id}\", \"content\": narrative}\n",
    "\n",
    "synthetic_tickets = [make_ticket(i) for i in range(NUM_TICKETS)]\n",
    "\n",
    "# 3.5 Aggregate all documents\n",
    "if not policy_docs and not synthetic_tickets:\n",
    "    # fallback minimal set\n",
    "    documents = [\n",
    "        {\"id\": \"fallback_baggage\", \"content\": \"Baggage allowance is 15 kg for economy and 23 kg for flex fares.\"},\n",
    "        {\"id\": \"fallback_cancellation\", \"content\": \"Cancellations within 24 hours are refunded if departure is at least 7 days away.\"},\n",
    "    ]\n",
    "else:\n",
    "    documents = policy_docs + synthetic_tickets\n",
    "\n",
    "print(f\"Policies: {len(policy_docs)}, Tickets: {len(synthetic_tickets)}, Total docs: {len(documents)}\")\n",
    "print(\"Sample doc IDs:\", [d['id'] for d in documents[:5]])\n",
    "# Quick diversity check\n",
    "unique_fares = {fare for d in documents if ' class.' in d['content'] for fare in FARES if fare.lower() in d['content'].lower()}\n",
    "print(\"Detected fare classes in tickets:\", sorted(unique_fares))\n",
    "assert documents, \"No documents ingested.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8069a",
   "metadata": {},
   "source": [
    "## 3. Ingestion (read PDFs + CSV)\n",
    "Reads from `dummy_files/` (PDF + CSV). If the directory or supported files are missing, falls back to a tiny synthetic corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9570fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini mode active: False (manual key provided=False)\n"
     ]
    }
   ],
   "source": [
    "# 2. Manual Gemini Key (optional) / LLM Mode Toggle\n",
    "use_gemini = False  # change to True to use Gemini\n",
    "manual_gemini_key = \"\"  # Paste key here if use_gemini=True (environment ignored for clarity)\n",
    "if use_gemini:\n",
    "    if not manual_gemini_key.strip():\n",
    "        raise EnvironmentError(\"Gemini mode enabled but no manual key provided.\")\n",
    "    if not HAS_GEMINI_LIB:\n",
    "        raise ImportError(\"google-generativeai not installed; can't use Gemini.\")\n",
    "    genai.configure(api_key=manual_gemini_key.strip())\n",
    "print(f\"Gemini mode active: {use_gemini} (manual key provided={bool(manual_gemini_key.strip())})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c82e69",
   "metadata": {},
   "source": [
    "## 2. Manual Gemini Key (optional) / LLM Mode Toggle\n",
    "Set `use_gemini = True` only if you paste a key below. Otherwise a deterministic mock LLM is used so the pipeline always runs offline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a700b43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer: False, FAISS: False, Gemini lib: False, PDF: False\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import os, re, math, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer  # type: ignore\n",
    "    HAS_ST = True\n",
    "except Exception:\n",
    "    SentenceTransformer = None  # type: ignore\n",
    "    HAS_ST = False\n",
    "\n",
    "try:\n",
    "    import faiss  # type: ignore\n",
    "    HAS_FAISS = True\n",
    "except Exception:\n",
    "    faiss = None  # type: ignore\n",
    "    HAS_FAISS = False\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai  # type: ignore\n",
    "    HAS_GEMINI_LIB = True\n",
    "except Exception:\n",
    "    genai = None\n",
    "    HAS_GEMINI_LIB = False\n",
    "\n",
    "try:\n",
    "    from PyPDF2 import PdfReader  # type: ignore\n",
    "    HAS_PDF = True\n",
    "except Exception:\n",
    "    PdfReader = None  # type: ignore\n",
    "    HAS_PDF = False\n",
    "\n",
    "np.random.seed(123)\n",
    "print(f\"SentenceTransformer: {HAS_ST}, FAISS: {HAS_FAISS}, Gemini lib: {HAS_GEMINI_LIB}, PDF: {HAS_PDF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab5cd699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt written. Install with: pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# 0. Requirements (write requirements.txt)\n",
    "requirements = \"\"\"numpy\n",
    "pandas\n",
    "sentence-transformers>=2.2 # optional, will fallback if missing\n",
    "faiss-cpu>=1.7 # optional\n",
    "google-generativeai>=0.3.0 # only if you manually supply a key\n",
    "pypdf2\n",
    "\"\"\"\n",
    "with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(requirements)\n",
    "print(\"requirements.txt written. Install with: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d77e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini active: False\n"
     ]
    }
   ],
   "source": [
    "# 3. Manual Gemini Key (optional) / LLM toggle\n",
    "use_gemini = False  # Keep False unless you paste a key below\n",
    "manual_gemini_key = \"\"  # Paste key here if using Gemini; leave blank for mock\n",
    "\n",
    "if use_gemini:\n",
    "    if not manual_gemini_key.strip():\n",
    "        raise EnvironmentError(\"Gemini mode enabled but no manual key provided.\")\n",
    "    if not HAS_GEMINI_LIB:\n",
    "        raise ImportError(\"google-generativeai not installed.\")\n",
    "    genai.configure(api_key=manual_gemini_key.strip())\n",
    "print(f\"Gemini active: {use_gemini}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d111f",
   "metadata": {},
   "source": [
    "(legacy cell removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652cbd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPDF2 not installed; skipping PDF extraction (add pypdf2 to requirements & reinstall).\n",
      "Loaded 20 docs (PDF pages collapsed + CSV rows). Example IDs: ['ticketing_info_row0', 'ticketing_info_row1', 'ticketing_info_row2']\n"
     ]
    }
   ],
   "source": [
    "# 3. Ingestion (PDF + CSV with fallback)\n",
    "# (updated already above - retained for execution ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92ce2b",
   "metadata": {},
   "source": [
    "### Simple Preprocessing\n",
    "Lowercase + collapse whitespace (minimal example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b89cc4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sample: ticketid=1000; pnr=y6dpbh; airline=indigo; flightno=6e125; departuredatetime=2025-12-09 07:15; origin=blr; destination=d ...\n"
     ]
    }
   ],
   "source": [
    "# 5. Preprocessing\n",
    "import re\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "for d in documents:\n",
    "    d['cleaned'] = clean(d['content'])\n",
    "print('Cleaned sample:', documents[0]['cleaned'][:120], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12908b2c",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Use local SentenceTransformer model for document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4ec210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (20, 159)\n"
     ]
    }
   ],
   "source": [
    "# 6. Embeddings (SentenceTransformer or TF-IDF fallback)\n",
    "class TFIDFEmbedder:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        from collections import Counter\n",
    "        self.tok = lambda s: [w for w in re.split(r\"[^a-z0-9]+\", s) if w]\n",
    "        token_lists = [self.tok(t) for t in texts]\n",
    "        df = Counter()\n",
    "        for toks in token_lists:\n",
    "            for t in set(toks):\n",
    "                df[t] += 1\n",
    "        self.vocab = {w: i for i, (w, _) in enumerate(df.items())}\n",
    "        n = len(texts)\n",
    "        self.idf = {w: math.log((1 + n) / (1 + c)) + 1 for w, c in df.items()}\n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        m = np.zeros((len(texts), len(self.vocab)), dtype='float32')\n",
    "        for i, txt in enumerate(texts):\n",
    "            toks = self.tok(txt)\n",
    "            if not toks: continue\n",
    "            tf = {}\n",
    "            for t in toks: tf[t] = tf.get(t, 0) + 1\n",
    "            for t, c in tf.items():\n",
    "                if t in self.vocab:\n",
    "                    col = self.vocab[t]\n",
    "                    m[i, col] = (c/len(toks))*self.idf[t]\n",
    "            nrm = np.linalg.norm(m[i]) + 1e-9\n",
    "            m[i] /= nrm\n",
    "        return m\n",
    "\n",
    "if HAS_ST:\n",
    "    st_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    def embed(texts: List[str]) -> np.ndarray:\n",
    "        v = st_model.encode(texts, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=False)\n",
    "        return v.astype('float32')\n",
    "else:\n",
    "    tfidf_model = TFIDFEmbedder([d['cleaned'] for d in documents])\n",
    "    def embed(texts: List[str]) -> np.ndarray:  # type: ignore\n",
    "        return tfidf_model.encode(texts)\n",
    "\n",
    "corpus_texts = [d['cleaned'] for d in documents]\n",
    "embeddings = embed(corpus_texts)\n",
    "print('Embeddings shape:', embeddings.shape)\n",
    "assert embeddings.shape[0] == len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b403016",
   "metadata": {},
   "source": [
    "## Vector Index (FAISS)\n",
    "Normalize vectors and use inner product as cosine similarity approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10841dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using brute-force numpy similarity.\n"
     ]
    }
   ],
   "source": [
    "# 7. Vector Index (normalize + optional FAISS)\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    return x / (np.linalg.norm(x, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "norm_embeddings = l2_normalize(embeddings)\n",
    "if HAS_FAISS:\n",
    "    index = faiss.IndexFlatIP(norm_embeddings.shape[1])\n",
    "    index.add(norm_embeddings)\n",
    "    print('FAISS index built:', index.ntotal)\n",
    "else:\n",
    "    index = None\n",
    "    print('Using brute-force numpy similarity.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9eeb9",
   "metadata": {},
   "source": [
    "### Retrieval Function\n",
    "Embed query, search top-k, return scored docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c091c743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test retrieval: [('ticket_2051', 0.112), ('ticket_2044', 0.109)]\n"
     ]
    }
   ],
   "source": [
    "# 8. Retrieval Test\n",
    "\n",
    "def embed_query(q: str) -> np.ndarray:\n",
    "    return l2_normalize(embed([clean(q)])).astype('float32')\n",
    "\n",
    "def retrieve(query: str, k: int = 3):\n",
    "    qv = embed_query(query)\n",
    "    if HAS_FAISS and index is not None:\n",
    "        scores, idxs = index.search(qv, k)\n",
    "        return [{\"id\": documents[i]['id'], \"score\": float(scores[0][j]), \"content\": documents[i]['content']} for j,i in enumerate(idxs[0])]\n",
    "    sims = (norm_embeddings @ qv.T).ravel()\n",
    "    order = np.argsort(-sims)[:k]\n",
    "    return [{\"id\": documents[i]['id'], \"score\": float(sims[i]), \"content\": documents[i]['content']} for i in order]\n",
    "\n",
    "print('Test retrieval:', [(r['id'], round(r['score'],3)) for r in retrieve('baggage allowance', k=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dfc14",
   "metadata": {},
   "source": [
    "## LLM Client Wrapper\n",
    "Mock mode (default) + optional Gemini mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee84bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test: (Mock) CONTEXT: baggage fees vary. QUESTION: What varies?\n"
     ]
    }
   ],
   "source": [
    "# 9. LLM Wrapper (mock or Gemini)\n",
    "class LLMClient:\n",
    "    def __init__(self, use_gemini: bool=False):\n",
    "        self.use_gemini = use_gemini\n",
    "        if self.use_gemini and (not HAS_GEMINI_LIB or not manual_gemini_key.strip()):\n",
    "            raise RuntimeError('Gemini requested but library or manual key missing.')\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        if not self.use_gemini:\n",
    "            question_part = prompt.split('QUESTION:')[-1].lower()\n",
    "            keywords = [w for w in re.split(r\"[^a-z0-9]+\", question_part) if len(w)>4][:6]\n",
    "            lines = [l.strip() for l in prompt.split('\\n') if l.strip()]\n",
    "            chosen = [l for l in lines if any(k in l.lower() for k in keywords)][:5]\n",
    "            synthesis = ' '.join(chosen) or lines[-1]\n",
    "            return f\"(Mock) {synthesis[:400]}\"\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        resp = model.generate_content(prompt)\n",
    "        return getattr(resp, 'text', str(resp))\n",
    "\n",
    "llm = LLMClient(use_gemini=use_gemini)\n",
    "print('LLM test:', llm.generate('CONTEXT: baggage fees vary. QUESTION: What varies?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7e84b",
   "metadata": {},
   "source": [
    "## RAG Prompt Assembly & Answer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f40ea01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Mock) Answer ONLY from context; if unknown say 'unsure'. [Doc policy_terms] Liability Limits: Compensation for baggage loss or damage is limited per kilogram u\n"
     ]
    }
   ],
   "source": [
    "# 10. RAG Assembly\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Answer ONLY from context; if unknown say 'unsure'.\\n\"\\\n",
    "    \"CONTEXT:\\n{context}\\n\\nQUESTION: {question}\\nANSWER:\"\n",
    ")\n",
    "\n",
    "def build_prompt(question: str, docs: List[Dict[str, Any]]) -> str:\n",
    "    blocks = [f\"[Doc {d['id']}]\\n{d['content']}\" for d in docs]\n",
    "    return PROMPT_TEMPLATE.format(context='\\n\\n'.join(blocks), question=question)\n",
    "\n",
    "def rag(question: str, k: int=3) -> Dict[str, Any]:\n",
    "    docs = retrieve(question, k=k)\n",
    "    prompt = build_prompt(question, docs)\n",
    "    answer = llm.generate(prompt)\n",
    "    return {\"question\": question, \"answer\": answer, \"docs\": docs, \"prompt\": prompt}\n",
    "\n",
    "print(rag('What is baggage allowance policy?', k=2)['answer'][:160])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea36953",
   "metadata": {},
   "source": [
    "## End-to-End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b9c8667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: How are cancellations handled and what about baggage limits?\n",
      "\n",
      "RETRIEVED:\n",
      "- policy_baggage score=0.276\n",
      "- policy_terms score=0.126\n",
      "- policy_cancellation score=0.079\n",
      "- ticket_2051 score=0.011\n",
      "\n",
      "ANSWER:\n",
      " (Mock) Answer ONLY from context; if unknown say 'unsure'. [Doc policy_baggage] Airline Baggage Policy (Effective 2025 Q3) Checked Allowance: Standard economy fares include 15 kg total checked baggage on domestic routes and 23 kg on international routes. FLEX and BUSINESS fares include an additional 10 kg allowance. Any single bag over 32 kg will not be accepted due to safety restrictions. Prohibited & Re\n"
     ]
    }
   ],
   "source": [
    "# 11. End-to-End Example\n",
    "question = 'How are cancellations handled and what about baggage limits?'\n",
    "result = rag(question, k=4)\n",
    "print('QUESTION:', result['question'])\n",
    "print('\\nRETRIEVED:')\n",
    "for d in result['docs']:\n",
    "    print(f\"- {d['id']} score={d['score']:.3f}\")\n",
    "print('\\nANSWER:\\n', result['answer'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd8b4e",
   "metadata": {},
   "source": [
    "## Additional Sample Questions\n",
    "Below are several general user-style questions to exercise the RAG pipeline across baggage, fare rules, cancellations, delays, and safety policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18ea3f",
   "metadata": {},
   "source": [
    "## Evaluation / Sanity Checks\n",
    "Basic asserts to verify pipeline integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b0bb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer ONLY from context; if unknown say 'unsure'.\n",
      "CONTEXT:\n",
      "[Doc ticketing_info_row0]\n",
      "TicketID=1000; PNR=Y6DPBH; Airline=IndiGo; FlightNo=6E125; DepartureDateTime=2025-12-09 07:15; Origin=BLR; Destination=DXB; PassengerName=Emily Davis; Price(INR)=9000; Status=Booked\n",
      "\n",
      "[Doc ticketing_info_row1]\n",
      "TicketID=1001; PNR=K51FPK; Airline=IndiGo; FlightNo=6E833; DepartureDateTime=2025-11-23 17:45; Origin=HYD; Destination=AMD; PassengerName=Harper Patel; Price(INR)=4500; Status=Cancelled\n",
      "\n",
      "[Doc ticketing_info_row2]\n",
      "TicketID=1002; PNR=0T9NT3; Airline=IndiGo; FlightNo=6E194; DepartureDateTime=2025-07-04 11:3\n"
     ]
    }
   ],
   "source": [
    "# 11.5 (Optional) Inspect Prompt\n",
    "print(result['prompt'][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e2c3430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the checked baggage allowance for flex and business fares?\n",
      "Top Docs: policy_baggage, policy_terms, policy_cancellation, ticket_2025\n",
      "Answer: (Mock) Answer ONLY from context; if unknown say 'unsure'. [Doc policy_baggage] Airline Baggage Policy (Effective 2025 Q3) Cabin (Carry-On): Each passenger may bring one cabin bag up to 7 kg and a pers\n",
      "---\n",
      "Q: How do cancellation rules differ between basic and flex tickets?\n",
      "Top Docs: policy_baggage, policy_terms, policy_cancellation, ticket_2016\n",
      "Answer: (Mock) Travel Documents: Passengers are responsible for valid passports, visas, and health certificates. Denied boarding due to documentation issues does not entitle the traveler to a refund beyond fa\n",
      "---\n",
      "Q: If my flight is delayed more than 3 hours what compensation or options do I have?\n",
      "Top Docs: policy_cancellation, policy_terms, ticket_2076, ticket_2007\n",
      "Answer: (Mock) Answer ONLY from context; if unknown say 'unsure'. 24-Hour Cooling-Off: Bookings made directly through our website can be fully refunded if canceled within 24 hours, provided departure is at le\n",
      "---\n",
      "Q: Can I change a standard fare ticket and what fees might apply?\n",
      "Top Docs: policy_cancellation, policy_baggage, policy_terms, ticket_2025\n",
      "Answer: (Mock) Answer ONLY from context; if unknown say 'unsure'. Ticket Change & Cancellation Policy (Updated July 2025) - BASIC: Non-refundable after cooling-off. Changes not permitted or require full reiss\n",
      "---\n",
      "Q: What safety or conduct rules could get a passenger denied boarding?\n",
      "Top Docs: policy_terms, policy_cancellation, policy_baggage, ticket_2051\n",
      "Answer: (Mock) Travel Documents: Passengers are responsible for valid passports, visas, and health certificates. Denied boarding due to documentation issues does not entitle the traveler to a refund beyond fa\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Additional RAG queries (diverse coverage)\n",
    "sample_questions = [\n",
    "    \"What is the checked baggage allowance for flex and business fares?\",\n",
    "    \"How do cancellation rules differ between basic and flex tickets?\",\n",
    "    \"If my flight is delayed more than 3 hours what compensation or options do I have?\",\n",
    "    \"Can I change a standard fare ticket and what fees might apply?\",\n",
    "    \"What safety or conduct rules could get a passenger denied boarding?\",\n",
    "]\n",
    "\n",
    "multi_results = []\n",
    "for q in sample_questions:\n",
    "    r = rag(q, k=4)\n",
    "    multi_results.append(r)\n",
    "    top_ids = \", \".join([d['id'] for d in r['docs']])\n",
    "    print(f\"Q: {q}\\nTop Docs: {top_ids}\\nAnswer: {r['answer'][:200]}\\n---\")\n",
    "\n",
    "# Basic assertion: each query returned at least one doc\n",
    "assert all(mr['docs'] for mr in multi_results), \"One of the sample questions returned no documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc272c",
   "metadata": {},
   "source": [
    "## 13. Notes / Next Steps\n",
    "- Add document chunking for long PDFs.\n",
    "- Persist vectors (FAISS index save or use a vector DB like Chroma / pgvector).\n",
    "- Add metadata filters (doc type: pdf, csv).\n",
    "- Introduce reranking or hybrid search.\n",
    "- Log queries & answers for evaluation.\n",
    "- Add citations highlighting which doc lines support each answer.\n",
    "\n",
    "Done — you've built a minimal RAG pipeline over airline policy docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f62695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
